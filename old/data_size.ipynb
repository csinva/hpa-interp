{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "import sys, os\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = os.getcwd()\n",
    "lib_path = opj(this_dir, 'bestfitting/protein_clean/src')\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on ip-172-31-62-205\n"
     ]
    }
   ],
   "source": [
    "import train_cls_net # import Protein class\n",
    "from net import _init_paths\n",
    "from config.config import * # set directory paths (DATA_DIR, RESULT_DIR etc)\n",
    "from dataset import protein_dataset # import ProteinDataset class\n",
    "from utils.augment_util import * # import augmentation functions\n",
    "from net.loss_funcs.kaggle_metric import prob_to_result # import prob_to_result\n",
    "from net.loss_funcs.kaggle_metric import get_probs_f1_score # import get_probs_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of images and labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/tif/train/tifs'\n",
    "img_resize_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/images_1536'\n",
    "\n",
    "# load image file names\n",
    "img = np.sort(os.listdir(img_dir))\n",
    "img_resize = np.sort(os.listdir(img_resize_dir))\n",
    "\n",
    "# load label name\n",
    "fname = opj(DATA_DIR, 'split/train_31072.csv')\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/train_31072.csv : (31072, 31)\n",
      "31072\n",
      "31072\n"
     ]
    }
   ],
   "source": [
    "# print file sizes\n",
    "print(fname,':', df.values.shape)\n",
    "print(int(img.shape[0]/4))\n",
    "print(int(img_resize.shape[0]/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_img_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/externel/HPAv18/jpg'\n",
    "external_img_resize_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536'\n",
    "\n",
    "# load file names\n",
    "external_img = np.sort(os.listdir(external_img_dir))\n",
    "external_resize = np.sort(os.listdir(external_img_resize_dir))\n",
    "\n",
    "# load label name\n",
    "fname = opj(DATA_DIR, 'split/external_74606.csv')\n",
    "df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/external_74606.csv : (74606, 33)\n",
      "74606\n",
      "74606\n"
     ]
    }
   ],
   "source": [
    "# print file sizes\n",
    "print(fname,':', df.values.shape)\n",
    "print(int(external_img.shape[0]/4))\n",
    "print(int(external_resize.shape[0]/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_folds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_train_cv0.csv : (24922, 31)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_train_cv1.csv : (24801, 31)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_train_cv2.csv : (24822, 31)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_train_cv3.csv : (24893, 31)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_train_cv4.csv : (24850, 31)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_valid_cv0.csv : (6150, 31) ;sum: 31072\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_valid_cv1.csv : (6271, 31) ;sum: 31072\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_valid_cv2.csv : (6250, 31) ;sum: 31072\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_valid_cv3.csv : (6179, 31) ;sum: 31072\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_folds5/random_valid_cv4.csv : (6222, 31) ;sum: 31072\n"
     ]
    }
   ],
   "source": [
    "split_alias = 'random_folds5'\n",
    "num = []\n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_train_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape)\n",
    "    num.append(df.values.shape[0])\n",
    "    \n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_valid_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape, ';sum:', num[fold] + df.values.shape[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_ext_folds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_train_cv0.csv : (84587, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_train_cv1.csv : (84563, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_train_cv2.csv : (84485, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_train_cv3.csv : (84588, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_train_cv4.csv : (84489, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_valid_cv0.csv : (21091, 33) sum: 105678\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_valid_cv1.csv : (21115, 33) sum: 105678\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_valid_cv2.csv : (21193, 33) sum: 105678\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_valid_cv3.csv : (21090, 33) sum: 105678\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_folds5/random_valid_cv4.csv : (21189, 33) sum: 105678\n"
     ]
    }
   ],
   "source": [
    "split_alias = 'random_ext_folds5'\n",
    "num = []\n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_train_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape)\n",
    "    num.append(df.values.shape[0])\n",
    "    \n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_valid_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape, 'sum:', num[fold] + df.values.shape[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_ext_noleak_clean_folds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_train_cv0.csv : (82747, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_train_cv1.csv : (82709, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_train_cv2.csv : (82643, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_train_cv3.csv : (82765, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_train_cv4.csv : (82660, 33)\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_valid_cv0.csv : (19984, 33) sum: 102731\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_valid_cv1.csv : (20062, 33) sum: 102771\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_valid_cv2.csv : (20202, 33) sum: 102845\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_valid_cv3.csv : (20060, 33) sum: 102825\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5/random_valid_cv4.csv : (20153, 33) sum: 102813\n"
     ]
    }
   ],
   "source": [
    "split_alias = 'random_ext_noleak_clean_folds5'\n",
    "num = []\n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_train_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape)\n",
    "    num.append(df.values.shape[0])\n",
    "    \n",
    "for fold in np.arange(5):\n",
    "    fname = opj(DATA_DIR, 'split', split_alias, \"random_valid_cv{}.csv\".format(fold))\n",
    "    df = pd.read_csv(fname)\n",
    "    print(fname,':', df.values.shape, 'sum:', num[fold] + df.values.shape[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = 'external'\n",
    "# fname = opj(DATA_DIR, 'meta/%s_meta.csv'%(header))\n",
    "# df = pd.read_csv(fname)\n",
    "# print(fname,':', df.values.shape)\n",
    "\n",
    "# # image ids for external dataset \n",
    "# external_id = df['Id'].values\n",
    "# external_id = np.sort(external_id)\n",
    "\n",
    "# # image directory\n",
    "# external_orig_img_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/externel/HPAv18/jpg'\n",
    "# external_orig2_img_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/externel/HPAv18_2/jpg'\n",
    "# external_resize_img_dir = '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536'\n",
    "\n",
    "# # load file names\n",
    "# external_orig = np.sort(os.listdir(external_orig_img_dir))\n",
    "# external_orig2 = np.sort(os.listdir(external_orig2_img_dir))\n",
    "# external_resize = np.sort(os.listdir(external_resize_img_dir))\n",
    "\n",
    "# # find images which are not in external_reize \n",
    "# idx = np.argwhere(~np.isin(external_orig, external_resize)).ravel()\n",
    "# wrong_fnames = external_orig[idx]\n",
    "# print(len(wrong_fnames))\n",
    "\n",
    "# # check if wrong files in external_orig are correct in external_orig2\n",
    "# num = len(wrong_fnames)\n",
    "# none_list = np.empty(num, dtype=bool)\n",
    "# none_checking = np.empty(num, dtype=bool)\n",
    "\n",
    "# for i in range(num):\n",
    "#     fname = wrong_fnames[i]\n",
    "#     color = fname.replace('.jpg','').split('_')[-1]\n",
    "#     im = cv2.imread(opj(external_orig_img_dir, fname))\n",
    "#     if im is None:\n",
    "#         none_list[i] = True\n",
    "#     else:\n",
    "#         none_list[i] = False\n",
    "        \n",
    "#     im_checking = cv2.imread(opj(external_orig2_img_dir, fname))\n",
    "#     if im_checking is None:\n",
    "#         none_checking[i] = True\n",
    "#     else:\n",
    "#         none_checking[i] = False\n",
    "# print(none_checking)\n",
    "\n",
    "# import shutil\n",
    "# # remove wrong files in external_orig and move from external_orig2\n",
    "# for i in range(num):\n",
    "#     fname = wrong_fnames[i]\n",
    "#     os.remove(opj(external_orig_img_dir, fname))\n",
    "#     shutil.move(opj(external_orig2_img_dir, fname), opj(external_orig_img_dir, fname))\n",
    "\n",
    "#     ### rerun resize code after moving files ###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
