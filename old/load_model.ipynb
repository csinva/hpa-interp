{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "import sys, os\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = os.getcwd()\n",
    "lib_path = opj(this_dir, 'bestfitting/protein_clean/src')\n",
    "if lib_path not in sys.path:\n",
    "    sys.path.insert(0, lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on ip-172-31-62-205\n"
     ]
    }
   ],
   "source": [
    "import train_cls_net # import Protein class\n",
    "from net import _init_paths\n",
    "from config.config import * # set directory paths (DATA_DIR, RESULT_DIR etc)\n",
    "from dataset import protein_dataset # import ProteinDataset class\n",
    "from utils.augment_util import * # import augmentation functions\n",
    "from net.loss_funcs.kaggle_metric import prob_to_result # import prob_to_result\n",
    "from net.loss_funcs.kaggle_metric import get_probs_f1_score # import get_probs_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = 'densenet'\n",
    "model_name = 'class_densenet121_large_dropout'\n",
    "out_dir = 'external_crop1024_focal_slov_hardlog_clean'\n",
    "train_batch_size = 36\n",
    "test_batch_size = 12\n",
    "scheduler_name = 'Adam45'\n",
    "img_size = 1536\n",
    "crop_size = 1024\n",
    "in_channels = 4\n",
    "gpus = '0' # which gpu to use\n",
    "\n",
    "folds_num = 5\n",
    "fold = 0\n",
    "num_classes = 28\n",
    "\n",
    "seed = 0\n",
    "aug_version = 2\n",
    "loss_name = 'FocalSymmetricLovaszHardLogLoss'\n",
    "predict_aug = 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose' # augmentation functions\n",
    "\n",
    "save_probs = True\n",
    "clipnorm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define dir_args and data_args as an input to Protein class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory arguments\n",
    "dir_args = {\n",
    "    \"split_dir\": opj(DATA_DIR, \"split\"), # directory to locate labels\n",
    "    \"log_dir\": opj(RESULT_DIR, \"logs\"),\n",
    "    \"subm_dir\": opj(RESULT_DIR, \"submissions\"),\n",
    "    \"model_dir\": opj(RESULT_DIR, \"models\"),\n",
    "    \"image_check_dir\": opj(RESULT_DIR, \"image_check\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "data_infos = {\n",
    "    \"model_level_name\": \"%s_i%d_aug%d_%dfolds/fold%d\" % (model_name if out_dir is None else out_dir + '_' + model_name,\n",
    "                                                             img_size, aug_version, folds_num, fold),\n",
    "}\n",
    "\n",
    "data_infos[\"train_split_file\"] = \"train_160.csv\"\n",
    "data_infos[\"valid_split_file\"] = \"valid_160.csv\"\n",
    "data_infos[\"test_split_file\"] = \"train_31072.csv\" \n",
    "\n",
    "# input of trainer.set_datasets\n",
    "data_args = {\n",
    "    \"train_split_file\": data_infos[\"train_split_file\"],\n",
    "    \"valid_split_file\": data_infos[\"valid_split_file\"],\n",
    "    \"test_split_file\": data_infos[\"test_split_file\"], \n",
    "    \"model_level_name\": data_infos[\"model_level_name\"],\n",
    "    \"result_type\": \"test\", \n",
    "    'predict_aug':predict_aug,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Protein class\n",
    "trainer = train_cls_net.Protein(dir_args,\n",
    "                                train_batch_size=train_batch_size,\n",
    "                                test_batch_size=test_batch_size,\n",
    "                                seed=seed, img_size=img_size,in_channels=in_channels,\n",
    "                                save_probs=save_probs,\n",
    "                                aug_version=aug_version,\n",
    "                                num_classes=num_classes,\n",
    "                                crop_size=crop_size,\n",
    "                                clipnorm=clipnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 4\n"
     ]
    }
   ],
   "source": [
    "# directory for densenet architecture\n",
    "model = import_module(\"net.%s\" % module)\n",
    "\n",
    "# get densenet architecture pretrained on imagenet (model_name = class_densenet121_large_dropout)\n",
    "net, scheduler, loss = model.get_model(model_name,\n",
    "                                       num_classes,\n",
    "                                       loss_name,\n",
    "                                       scheduler_name=scheduler_name,\n",
    "                                       in_channels=in_channels,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model file from: /home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/models/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/final.pth\n",
      "using gpu 0\n"
     ]
    }
   ],
   "source": [
    "# set directories for train, valid, test datasets and to save results\n",
    "trainer.set_datasets(data_args)\n",
    "\n",
    "# load model from model file\n",
    "trainer.load_model(net=net, epoch=None) \n",
    "\n",
    "# print model file \n",
    "print('load model file from:', trainer.get_model_file()) \n",
    "\n",
    "# number of GPUs to use\n",
    "n_gpu = trainer.setgpu(gpus)\n",
    "net = trainer.set_data_parallel(net, n_gpu=n_gpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/images_1536\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# which file to evaluate #\n",
    "data_infos[\"test_split_file\"] = \"random_ext_folds5/random_valid_cv4.csv\"\n",
    "\n",
    "data_args = {\n",
    "    \"train_split_file\": data_infos[\"train_split_file\"],\n",
    "    \"valid_split_file\": data_infos[\"valid_split_file\"],\n",
    "    \"test_split_file\": data_infos[\"test_split_file\"], \n",
    "    \"model_level_name\": data_infos[\"model_level_name\"],\n",
    "    \"result_type\": \"test\", \n",
    "    'predict_aug':predict_aug,\n",
    "}\n",
    "\n",
    "trainer.set_datasets(data_args)\n",
    "###########################\n",
    "\n",
    "# test dataset and dataloader\n",
    "test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "                                               img_size=trainer.img_size,\n",
    "                                               is_trainset=True, # if True, save labels to self.labels \n",
    "                                               return_label=True, # if True, return labels when indexing\n",
    "                                               seed=trainer.seed,\n",
    "                                               in_channels=trainer.in_channels,\n",
    "                                               transform=None,\n",
    "                                               crop_size=trainer.crop_size,\n",
    "                                               random_crop=trainer.seed!=0,\n",
    "                                               )\n",
    "\n",
    "test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "                                         sampler=SequentialSampler(test_dataset),\n",
    "                                         batch_size=trainer.test_batch_size,\n",
    "                                         drop_last=False,\n",
    "                                         num_workers=trainer.num_workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to evaluate\n"
     ]
    }
   ],
   "source": [
    "# list of augment functions\n",
    "augments = trainer.predict_aug.split(',') \n",
    "\n",
    "# pick one augmentation transform\n",
    "augment_name = augments[4]\n",
    "\n",
    "# set transform function\n",
    "test_dataset.transform = [eval('augment_%s' % augment_name)]\n",
    "\n",
    "# set directories for submission\n",
    "epoch_name = 'epoch_final'\n",
    "augment_name += '_seed%d'%seed\n",
    "sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "\n",
    "# where to store the results\n",
    "trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % 'val')\n",
    "trainer.result_prob_fname = opj(sub_dir, 'prob_%s.npy' % 'val')\n",
    "trainer.extract_feat_fname = opj(sub_dir, 'extract_feats_%s.npz' % 'val')\n",
    "\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "# use gpu\n",
    "if trainer.gpu_flag:\n",
    "    net.cuda()\n",
    "# net eval mode\n",
    "net.eval()\n",
    "\n",
    "print('ready to evaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score:0.65546\n"
     ]
    }
   ],
   "source": [
    "# count number of test points\n",
    "n = 0 \n",
    "\n",
    "# get img_ids from dataset\n",
    "img_ids = np.array(test_dataset.img_ids) \n",
    "\n",
    "# list for probs\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iter, (images, labels, indices) in tqdm(enumerate(test_loader, 0), total=int(np.ceil(test_dataset.num / trainer.test_batch_size))):\n",
    "        batch_size = len(images)\n",
    "        n += batch_size\n",
    "        if trainer.gpu_flag:\n",
    "            images = Variable(images.cuda(), volatile=True)\n",
    "        else:\n",
    "            images = Variable(images, volatile=True)\n",
    "\n",
    "        outputs = net(images)\n",
    "        logits = outputs\n",
    "\n",
    "        probs = trainer.logits_to_probs(logits.data)\n",
    "        all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "# save probability vectors\n",
    "all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "if trainer.save_probs:\n",
    "    np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "# save predicted labels\n",
    "df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "df.to_csv(trainer.result_csv_file, index=False, compression='gzip')\n",
    "\n",
    "# get F1-score\n",
    "truth = pd.read_csv(trainer.test_split_file)\n",
    "score = get_probs_f1_score(df, all_probs, truth, th=0.5)\n",
    "\n",
    "print('macro f1 score:%.5f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = df.copy()\n",
    "pred_results['Target'] = truth['Target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c99ba-bba4-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001bcdd2-bbb2-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002daad6-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0032a07e-bba9-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 24</td>\n",
       "      <td>0 24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00344514-bbc2-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id Predicted Target\n",
       "0  000c99ba-bba4-11e8-b2b9-ac1f6b6435d0         1      1\n",
       "1  001bcdd2-bbb2-11e8-b2ba-ac1f6b6435d0         0      0\n",
       "2  002daad6-bbc9-11e8-b2bc-ac1f6b6435d0         7      7\n",
       "3  0032a07e-bba9-11e8-b2ba-ac1f6b6435d0      0 24   0 24\n",
       "4  00344514-bbc2-11e8-b2bb-ac1f6b6435d0        23     23"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
