{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "import os\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bestfitting.protein_clean.src.net import _init_paths\n",
    "from bestfitting.protein_clean.src.net import densenet\n",
    "from bestfitting.protein_clean.src.config.config import *\n",
    "from bestfitting.protein_clean.src.dataset import protein_dataset\n",
    "from bestfitting.protein_clean.src import train_net_base\n",
    "from bestfitting.protein_clean.src import train_cls_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = 'densenet'\n",
    "model_name = 'class_densenet121_large_dropout'\n",
    "out_dir = 'external_crop1024_focal_slov_hardlog_clean'\n",
    "train_batch_size = 36\n",
    "test_batch_size = 12\n",
    "epochs = 45\n",
    "scheduler = 'Adam45'\n",
    "scheduler_name = scheduler\n",
    "img_size = 1536\n",
    "crop_size = 1024\n",
    "in_channels = 4\n",
    "start_fold = 0\n",
    "end_fold = 2\n",
    "# gpus = '0, 1, 2, 3'\n",
    "gpus = '0'\n",
    "\n",
    "folds_num = 5\n",
    "fold = 0\n",
    "num_classes = 28\n",
    "\n",
    "train_model = 0\n",
    "predict_val = 1\n",
    "predict_test = 1\n",
    "is_predict_val = True\n",
    "is_predict_test = True\n",
    "predict_aug = 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'\n",
    "seeds = '0,1,2,3'\n",
    "seed = 100\n",
    "aug_version = 2\n",
    "loss = 'FocalSymmetricLovaszHardLogLoss'\n",
    "loss_name = loss\n",
    "predict_epoch = None\n",
    "predict_epochs = [predict_epoch]\n",
    "\n",
    "save_probs = True\n",
    "clipnorm = True\n",
    "overwrite = True\n",
    "\n",
    "# use external data\n",
    "use_external = True\n",
    "\n",
    "# no leak\n",
    "clean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_alias = 'random'\n",
    "if use_external:\n",
    "    split_alias = 'random_ext'\n",
    "    if clean: # no leak in the data\n",
    "        split_alias = 'random_ext_noleak_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split_dir': '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/split/random_ext_noleak_clean_folds5', 'log_dir': '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/logs', 'subm_dir': '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/submissions', 'model_dir': '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/models', 'image_check_dir': '/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/image_check'}\n"
     ]
    }
   ],
   "source": [
    "# directory arguments\n",
    "dir_args = {\n",
    "    \"split_dir\": opj(DATA_DIR, \"split\", \"%s_folds%d\" % (split_alias, folds_num)),\n",
    "    \"log_dir\": opj(RESULT_DIR, \"logs\"),\n",
    "    \"subm_dir\": opj(RESULT_DIR, \"submissions\"),\n",
    "    \"model_dir\": opj(RESULT_DIR, \"models\"),\n",
    "    \"image_check_dir\": opj(RESULT_DIR, \"image_check\"),\n",
    "}\n",
    "print(dir_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'train_split_file': 'random_train_cv0.csv', 'valid_split_file': 'random_valid_cv0.csv', 'test_split_file': '../test_11702.csv'}\n"
     ]
    }
   ],
   "source": [
    "# data files\n",
    "data_infos = {\n",
    "    \"model_level_name\": \"%s_i%d_aug%d_%dfolds/fold%d\" % (model_name if out_dir is None else out_dir + '_' + model_name,\n",
    "                                                             img_size, aug_version, folds_num, fold),\n",
    "}\n",
    "is_debug = False\n",
    "if is_debug: # if true we use small dataset for debugging\n",
    "    data_infos[\"train_split_file\"] = \"../train_160.csv\"\n",
    "    data_infos[\"valid_split_file\"] = \"../valid_160.csv\"\n",
    "    data_infos[\"test_split_file\"] = \"../test_160.csv\"\n",
    "else:\n",
    "    data_infos[\"train_split_file\"] = \"random_train_cv{}.csv\".format(fold)\n",
    "    data_infos[\"valid_split_file\"] = \"random_valid_cv{}.csv\".format(fold)\n",
    "    data_infos[\"test_split_file\"] = \"../test_11702.csv\"\n",
    "print(data_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_split_file': 'random_train_cv0.csv', 'valid_split_file': 'random_valid_cv0.csv', 'test_split_file': 'random_valid_cv0.csv', 'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'result_type': 'val', 'predict_aug': 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'}\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "    \"train_split_file\": data_infos[\"train_split_file\"],\n",
    "    \"valid_split_file\": data_infos[\"valid_split_file\"],\n",
    "    \"test_split_file\": data_infos[\"valid_split_file\"], # should be \"test_split_file?\"\n",
    "    \"model_level_name\": data_infos[\"model_level_name\"],\n",
    "    \"result_type\": \"val\", # for test, should change to 'test'\n",
    "    'predict_aug':predict_aug,\n",
    "}\n",
    "print(data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Protein class\n",
    "trainer = train_cls_net.Protein(dir_args,\n",
    "                                train_batch_size=train_batch_size,\n",
    "                                test_batch_size=test_batch_size,\n",
    "                                seed=seed, img_size=img_size,in_channels=in_channels,\n",
    "                                save_probs=save_probs,\n",
    "                                aug_version=aug_version,\n",
    "                                num_classes=num_classes,\n",
    "                                crop_size=crop_size,\n",
    "                                use_external=use_external,\n",
    "                                clipnorm=clipnorm,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 0\n",
      "in_channels 4\n"
     ]
    }
   ],
   "source": [
    "# check how many gpus?\n",
    "n_gpu = trainer.setgpu(gpus)\n",
    "\n",
    "# directory for densenet architecture\n",
    "model = import_module(\"net.%s\" % module)\n",
    "\n",
    "# get densenet architecture pretrained on imagenet (model_name = class_densenet121_large_dropout)\n",
    "net, scheduler, loss = model.get_model(model_name,\n",
    "                                       num_classes,\n",
    "                                       loss_name,\n",
    "                                       scheduler_name=scheduler_name,\n",
    "                                       in_channels=in_channels,\n",
    "                                       )\n",
    "\n",
    "\n",
    "# GPU\n",
    "net = trainer.set_data_parallel(net, n_gpu=n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/models/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/final.pth\n"
     ]
    }
   ],
   "source": [
    "# set directories\n",
    "trainer.set_datasets(data_args)\n",
    "\n",
    "# load model located on model file\n",
    "trainer.load_model(net=net, epoch=predict_epoch) \n",
    "\n",
    "# print model file \n",
    "print(trainer.get_model_file()) \n",
    "\n",
    "# use GPU\n",
    "net = trainer.set_data_parallel(net, n_gpu=n_gpu) \n",
    "\n",
    "if seeds is not None:\n",
    "    seeds = [int(i) for i in seeds.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seed in seeds:\n",
    "#     trainer.seed = seed\n",
    "#     if is_predict_val:\n",
    "#         data_args['result_type'] = 'val'\n",
    "#         data_args['test_split_file'] = data_infos[\"valid_split_file\"]\n",
    "#         trainer.set_datasets(data_args)\n",
    "        \n",
    "#         # run prediction code\n",
    "#         trainer.do_submission(net, overwrite)\n",
    "\n",
    "#     if is_predict_test:\n",
    "#         data_args['result_type'] = 'test'\n",
    "#         data_args['test_split_file'] = data_infos[\"test_split_file\"]\n",
    "#         trainer.set_datasets(data_args)\n",
    "        \n",
    "#         # run prediction code\n",
    "#         trainer.do_submission(net, overwrite)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fliplr\n"
     ]
    }
   ],
   "source": [
    "augments = predict_aug.split(',')\n",
    "augment_name = augments[2]\n",
    "print(augment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/submissions/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/epoch_final/fliplr_seed100/results_val_external.csv.gz\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/submissions/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/epoch_final/fliplr_seed100/prob_val_external.npy\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/submissions/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/epoch_final/fliplr_seed100/extract_feats_val_external.npz\n"
     ]
    }
   ],
   "source": [
    "epoch_name = 'epoch_final'\n",
    "augment_name += '_seed%d'%seed\n",
    "sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "\n",
    "if trainer.use_external and trainer.result_type == 'val':\n",
    "    trainer.result_csv_file = opj(sub_dir, 'results_%s_external.csv.gz' % trainer.result_type)\n",
    "    trainer.result_prob_fname = opj(sub_dir, \"prob_%s_external.npy\" % trainer.result_type)\n",
    "    trainer.extract_feat_fname = opj(sub_dir, 'extract_feats_%s_external.npz' % trainer.result_type)\n",
    "else:\n",
    "    trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % trainer.result_type)\n",
    "    trainer.result_prob_fname = opj(sub_dir, \"prob_%s.npy\" % trainer.result_type)\n",
    "    trainer.extract_feat_fname = opj(sub_dir, 'extract_feats_%s.npz' % trainer.result_type)\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "print(trainer.result_csv_file)\n",
    "print(trainer.result_prob_fname)\n",
    "print(trainer.extract_feat_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/images_1536\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536\n"
     ]
    }
   ],
   "source": [
    "test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "                                               img_size=trainer.img_size,\n",
    "                                               is_trainset=not trainer.result_type == 'test',\n",
    "                                               return_label=True,\n",
    "                                               seed=trainer.seed,\n",
    "                                               in_channels=trainer.in_channels,\n",
    "                                               transform=None,\n",
    "                                               crop_size=trainer.crop_size,\n",
    "                                               random_crop=trainer.seed!=0,\n",
    "                                               )\n",
    "\n",
    "test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "                                         sampler=SequentialSampler(test_dataset),\n",
    "                                         batch_size=trainer.test_batch_size,\n",
    "                                         drop_last=False,\n",
    "                                         num_workers=trainer.num_workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1666 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 2.44 MiB free; 43.87 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-9f3654118da5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 2.44 MiB free; 43.87 MiB cached)"
     ]
    }
   ],
   "source": [
    "if trainer.gpu_flag:\n",
    "    net.cuda()\n",
    "# net eval mode\n",
    "net.eval()\n",
    "        \n",
    "n = 0 # number of test data points\n",
    "img_ids = np.array(test_dataset.img_ids) # get img_ids from dataset\n",
    "all_probs = []\n",
    "for iter, (images, labels, indices) in tqdm(enumerate(test_loader, 0), # return_label is False\n",
    "                                            total=int(np.ceil(test_dataset.num / trainer.test_batch_size))):\n",
    "    batch_size = len(images)\n",
    "    n += batch_size\n",
    "    if trainer.gpu_flag:\n",
    "        images = Variable(images.cuda(), volatile=True)\n",
    "    else:\n",
    "        images = Variable(images, volatile=True)\n",
    "\n",
    "    outputs = net(images)\n",
    "    if type(outputs)==list or type(outputs)==tuple:\n",
    "        logits = outputs[0]\n",
    "    else:\n",
    "        logits = outputs\n",
    "\n",
    "    probs = trainer.logits_to_probs(logits.data)\n",
    "    all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "# start = timer()\n",
    "\n",
    "all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "if trainer.save_probs:\n",
    "    print(all_probs.shape)\n",
    "    np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "df.to_csv(trainer.result_csv_file, index=False, compression='gzip')\n",
    "\n",
    "# if result_type is 'val', then compute f1_score\n",
    "if trainer.result_type == 'val':\n",
    "    truth = pd.read_csv(trainer.valid_split_file)\n",
    "    score = get_probs_f1_score(df, all_probs, truth, th=0.5)\n",
    "    print('macro f1 score:%.5f' % score)\n",
    "    if trainer.use_external:\n",
    "        sub_name = 'results_%s_%.5f_external.csv.gz' % (trainer.result_type, score)\n",
    "    else:\n",
    "        sub_name = 'results_%s_%.5f.csv.gz' % (trainer.result_type, score)\n",
    "    df.to_csv(opj(sub_dir, sub_name), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
