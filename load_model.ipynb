{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "import os\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bestfitting.protein_clean.src.net import _init_paths\n",
    "from bestfitting.protein_clean.src.net import densenet\n",
    "from bestfitting.protein_clean.src.config.config import *\n",
    "from bestfitting.protein_clean.src.dataset import protein_dataset\n",
    "from bestfitting.protein_clean.src import train_net_base\n",
    "from bestfitting.protein_clean.src import train_cls_net\n",
    "from bestfitting.protein_clean.src.utils.augment_util import *\n",
    "from bestfitting.protein_clean.src.net.loss_funcs.kaggle_metric import prob_to_result\n",
    "from net.tool import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = 'densenet'\n",
    "model_name = 'class_densenet121_large_dropout'\n",
    "out_dir = 'external_crop1024_focal_slov_hardlog_clean'\n",
    "train_batch_size = 36\n",
    "test_batch_size = 12\n",
    "epochs = 45\n",
    "scheduler = 'Adam45'\n",
    "scheduler_name = scheduler\n",
    "img_size = 1536\n",
    "crop_size = 1024\n",
    "in_channels = 4\n",
    "gpus = '0'\n",
    "\n",
    "folds_num = 5\n",
    "fold = 0\n",
    "num_classes = 28\n",
    "\n",
    "is_predict_val = True\n",
    "is_predict_test = True\n",
    "predict_aug = 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'\n",
    "seeds = '0,1,2,3'\n",
    "seed = 100\n",
    "aug_version = 2\n",
    "loss = 'FocalSymmetricLovaszHardLogLoss'\n",
    "loss_name = loss\n",
    "\n",
    "save_probs = True\n",
    "clipnorm = True\n",
    "overwrite = True\n",
    "\n",
    "# use external data\n",
    "use_external = False\n",
    "\n",
    "# no leak\n",
    "clean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n"
     ]
    }
   ],
   "source": [
    "split_alias = 'random'\n",
    "if use_external:\n",
    "    split_alias = 'random_ext'\n",
    "    if clean: # no leak in the data\n",
    "        split_alias = 'random_ext_noleak_clean'\n",
    "print(split_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory arguments\n",
    "dir_args = {\n",
    "    \"split_dir\": opj(DATA_DIR, \"split\", \"%s_folds%d\" % (split_alias, folds_num)),\n",
    "    \"log_dir\": opj(RESULT_DIR, \"logs\"),\n",
    "    \"subm_dir\": opj(RESULT_DIR, \"submissions\"),\n",
    "    \"model_dir\": opj(RESULT_DIR, \"models\"),\n",
    "    \"image_check_dir\": opj(RESULT_DIR, \"image_check\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'train_split_file': '../train_160.csv', 'valid_split_file': '../valid_160.csv', 'test_split_file': '../test_160.csv'}\n"
     ]
    }
   ],
   "source": [
    "# data files\n",
    "data_infos = {\n",
    "    \"model_level_name\": \"%s_i%d_aug%d_%dfolds/fold%d\" % (model_name if out_dir is None else out_dir + '_' + model_name,\n",
    "                                                             img_size, aug_version, folds_num, fold),\n",
    "}\n",
    "\n",
    "data_infos[\"train_split_file\"] = \"../train_160.csv\"\n",
    "data_infos[\"valid_split_file\"] = \"../valid_160.csv\"\n",
    "data_infos[\"test_split_file\"] = \"../test_160.csv\"\n",
    "print(data_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_split_file': '../train_160.csv', 'valid_split_file': '../valid_160.csv', 'test_split_file': '../valid_160.csv', 'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'result_type': 'val', 'predict_aug': 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'}\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "    \"train_split_file\": data_infos[\"train_split_file\"],\n",
    "    \"valid_split_file\": data_infos[\"valid_split_file\"],\n",
    "    \"test_split_file\": data_infos[\"valid_split_file\"], # should be \"test_split_file?\"\n",
    "    \"model_level_name\": data_infos[\"model_level_name\"],\n",
    "    \"result_type\": \"val\", # for test, should change to 'test'\n",
    "    'predict_aug':predict_aug,\n",
    "}\n",
    "print(data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Protein class\n",
    "trainer = train_cls_net.Protein(dir_args,\n",
    "                                train_batch_size=train_batch_size,\n",
    "                                test_batch_size=test_batch_size,\n",
    "                                seed=seed, img_size=img_size,in_channels=in_channels,\n",
    "                                save_probs=save_probs,\n",
    "                                aug_version=aug_version,\n",
    "                                num_classes=num_classes,\n",
    "                                crop_size=crop_size,\n",
    "                                use_external=use_external,\n",
    "                                clipnorm=clipnorm,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 4\n"
     ]
    }
   ],
   "source": [
    "# directory for densenet architecture\n",
    "model = import_module(\"net.%s\" % module)\n",
    "\n",
    "# get densenet architecture pretrained on imagenet (model_name = class_densenet121_large_dropout)\n",
    "net, scheduler, loss = model.get_model(model_name,\n",
    "                                       num_classes,\n",
    "                                       loss_name,\n",
    "                                       scheduler_name=scheduler_name,\n",
    "                                       in_channels=in_channels,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model file from: /home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/models/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/final.pth\n",
      "using gpu 0\n"
     ]
    }
   ],
   "source": [
    "# set directories\n",
    "trainer.set_datasets(data_args)\n",
    "\n",
    "# load model located on model file\n",
    "trainer.load_model(net=net, epoch=None) \n",
    "\n",
    "# print model file \n",
    "print('load model file from:', trainer.get_model_file()) \n",
    "\n",
    "# use GPU\n",
    "n_gpu = trainer.setgpu(gpus)\n",
    "net = trainer.set_data_parallel(net, n_gpu=n_gpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_predict_val = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/images_1536\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536\n",
      "iter:  1\n",
      "iter:  11\n"
     ]
    }
   ],
   "source": [
    "if is_predict_val:\n",
    "    data_args['result_type'] = 'val'\n",
    "    data_args['test_split_file'] = data_infos[\"valid_split_file\"]\n",
    "    trainer.set_datasets(data_args)\n",
    "\n",
    "    # test dataset and dataloader\n",
    "    test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "                                                   img_size=trainer.img_size,\n",
    "                                                   is_trainset=not trainer.result_type == 'test',\n",
    "                                                   return_label=True,\n",
    "                                                   seed=trainer.seed,\n",
    "                                                   in_channels=trainer.in_channels,\n",
    "                                                   transform=None,\n",
    "                                                   crop_size=trainer.crop_size,\n",
    "                                                   random_crop=trainer.seed!=0,\n",
    "                                                   )\n",
    "\n",
    "    test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "                                             sampler=SequentialSampler(test_dataset),\n",
    "                                             batch_size=trainer.test_batch_size,\n",
    "                                             drop_last=False,\n",
    "                                             num_workers=trainer.num_workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    augments = trainer.predict_aug.split(',')\n",
    "    augment_name = augments[2]\n",
    "    # define transform\n",
    "    test_dataset.transform = [eval('augment_%s' % augment_name)]\n",
    "    \n",
    "    epoch_name = 'epoch_final'\n",
    "    augment_name += '_seed%d'%seed\n",
    "    sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "    \n",
    "    if trainer.use_external and trainer.result_type == 'val':\n",
    "        trainer.result_csv_file = opj(sub_dir, 'results_%s_external.csv.gz' % trainer.result_type)\n",
    "        trainer.result_prob_fname = opj(sub_dir, \"prob_%s_external.npy\" % trainer.result_type)\n",
    "    else:\n",
    "        trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % trainer.result_type)\n",
    "        trainer.result_prob_fname = opj(sub_dir, \"prob_%s.npy\" % trainer.result_type)\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "    # use gpu\n",
    "    if trainer.gpu_flag:\n",
    "        net.cuda()\n",
    "    # net eval mode\n",
    "    net.eval()\n",
    "\n",
    "    n = 0 # number of test data points\n",
    "    img_ids = np.array(test_dataset.img_ids) # get img_ids from dataset\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for iter, (images, labels, indices) in enumerate(test_loader, 0):\n",
    "            if iter % 10 == 1:\n",
    "                print('iter: ', iter)\n",
    "            batch_size = len(images)\n",
    "            n += batch_size\n",
    "            if trainer.gpu_flag:\n",
    "                images = Variable(images.cuda(), volatile=True)\n",
    "            else:\n",
    "                images = Variable(images, volatile=True)\n",
    "\n",
    "            outputs = net(images)\n",
    "            logits = outputs\n",
    "\n",
    "            probs = trainer.logits_to_probs(logits.data)\n",
    "            all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "    # start = timer()\n",
    "\n",
    "    all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "    if trainer.save_probs:\n",
    "        print(all_probs.shape)\n",
    "        np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "    df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "    df.to_csv(trainer.result_csv_file, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(labels.numpy() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_predict_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_predict_test:\n",
    "#     data_args['result_type'] = 'test'\n",
    "#     data_args['test_split_file'] = data_infos[\"test_split_file\"]\n",
    "#     trainer.set_datasets(data_args)\n",
    "\n",
    "#     # test dataset and dataloader\n",
    "#     test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "#                                                    img_size=trainer.img_size,\n",
    "#                                                    is_trainset=not trainer.result_type == 'test',\n",
    "#                                                    return_label=True,\n",
    "#                                                    seed=trainer.seed,\n",
    "#                                                    in_channels=trainer.in_channels,\n",
    "#                                                    transform=None,\n",
    "#                                                    crop_size=trainer.crop_size,\n",
    "#                                                    random_crop=trainer.seed!=0,\n",
    "#                                                    )\n",
    "\n",
    "#     test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "#                                              sampler=SequentialSampler(test_dataset),\n",
    "#                                              batch_size=trainer.test_batch_size,\n",
    "#                                              drop_last=False,\n",
    "#                                              num_workers=trainer.num_workers,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "#     augments = trainer.predict_aug.split(',')\n",
    "#     augment_name = augments[2]\n",
    "#     # define transform\n",
    "#     test_dataset.transform = [eval('augment_%s' % augment_name)]\n",
    "    \n",
    "#     epoch_name = 'epoch_final'\n",
    "#     augment_name += '_seed%d'%seed\n",
    "#     sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "    \n",
    "#     if trainer.use_external and trainer.result_type == 'val':\n",
    "#         trainer.result_csv_file = opj(sub_dir, 'results_%s_external.csv.gz' % trainer.result_type)\n",
    "#         trainer.result_prob_fname = opj(sub_dir, \"prob_%s_external.npy\" % trainer.result_type)\n",
    "#     else:\n",
    "#         trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % trainer.result_type)\n",
    "#         trainer.result_prob_fname = opj(sub_dir, \"prob_%s.npy\" % trainer.result_type)\n",
    "#     os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "#     # use gpu\n",
    "#     if trainer.gpu_flag:\n",
    "#         net.cuda()\n",
    "#     # net eval mode\n",
    "#     net.eval()\n",
    "\n",
    "#     n = 0 # number of test data points\n",
    "#     img_ids = np.array(test_dataset.img_ids) # get img_ids from dataset\n",
    "#     all_probs = []\n",
    "#     with torch.no_grad():\n",
    "#         for iter, (images, labels, indices) in enumerate(test_loader, 0):\n",
    "#             if iter % 10 == 1:\n",
    "#                 print('iter: ', iter)\n",
    "#             batch_size = len(images)\n",
    "#             n += batch_size\n",
    "#             if trainer.gpu_flag:\n",
    "#                 images = Variable(images.cuda(), volatile=True)\n",
    "#             else:\n",
    "#                 images = Variable(images, volatile=True)\n",
    "\n",
    "#             outputs = net(images)\n",
    "#             logits = outputs\n",
    "\n",
    "#             probs = trainer.logits_to_probs(logits.data)\n",
    "#             all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "#     # start = timer()\n",
    "\n",
    "#     all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "#     if trainer.save_probs:\n",
    "#         print(all_probs.shape)\n",
    "#         np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "#     df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "#     df.to_csv(trainer.result_csv_file, index=False, compression='gzip')\n",
    "# print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
