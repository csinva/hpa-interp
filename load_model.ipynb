{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "import os\n",
    "opj = os.path.join\n",
    "ope = os.path.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bestfitting.protein_clean.src.net import _init_paths\n",
    "from bestfitting.protein_clean.src.net import densenet\n",
    "from bestfitting.protein_clean.src.config.config import *\n",
    "from bestfitting.protein_clean.src.dataset import protein_dataset\n",
    "from bestfitting.protein_clean.src import train_net_base\n",
    "from bestfitting.protein_clean.src import train_cls_net\n",
    "from bestfitting.protein_clean.src.utils.augment_util import *\n",
    "from bestfitting.protein_clean.src.net.loss_funcs.kaggle_metric import prob_to_result\n",
    "from net.tool import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = 'densenet'\n",
    "model_name = 'class_densenet121_large_dropout'\n",
    "out_dir = 'external_crop1024_focal_slov_hardlog_clean'\n",
    "train_batch_size = 36\n",
    "test_batch_size = 12\n",
    "epochs = 45\n",
    "scheduler = 'Adam45'\n",
    "scheduler_name = scheduler\n",
    "img_size = 1536\n",
    "crop_size = 1024\n",
    "in_channels = 4\n",
    "gpus = '0'\n",
    "\n",
    "folds_num = 5\n",
    "fold = 0\n",
    "num_classes = 28\n",
    "\n",
    "is_predict_val = True\n",
    "is_predict_test = True\n",
    "predict_aug = 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'\n",
    "seeds = '0,1,2,3'\n",
    "seed = 100\n",
    "aug_version = 2\n",
    "loss = 'FocalSymmetricLovaszHardLogLoss'\n",
    "loss_name = loss\n",
    "\n",
    "save_probs = True\n",
    "clipnorm = True\n",
    "overwrite = True\n",
    "\n",
    "# use external data\n",
    "use_external = True\n",
    "\n",
    "# no leak\n",
    "clean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_ext_noleak_clean\n"
     ]
    }
   ],
   "source": [
    "split_alias = 'random'\n",
    "if use_external:\n",
    "    split_alias = 'random_ext'\n",
    "    if clean: # no leak in the data\n",
    "        split_alias = 'random_ext_noleak_clean'\n",
    "print(split_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory arguments\n",
    "dir_args = {\n",
    "    \"split_dir\": opj(DATA_DIR, \"split\", \"%s_folds%d\" % (split_alias, folds_num)),\n",
    "    \"log_dir\": opj(RESULT_DIR, \"logs\"),\n",
    "    \"subm_dir\": opj(RESULT_DIR, \"submissions\"),\n",
    "    \"model_dir\": opj(RESULT_DIR, \"models\"),\n",
    "    \"image_check_dir\": opj(RESULT_DIR, \"image_check\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'train_split_file': '../train_160.csv', 'valid_split_file': '../valid_160.csv', 'test_split_file': '../test_160.csv'}\n"
     ]
    }
   ],
   "source": [
    "# data files\n",
    "data_infos = {\n",
    "    \"model_level_name\": \"%s_i%d_aug%d_%dfolds/fold%d\" % (model_name if out_dir is None else out_dir + '_' + model_name,\n",
    "                                                             img_size, aug_version, folds_num, fold),\n",
    "}\n",
    "is_debug = True\n",
    "if is_debug: # if true we use small dataset for debugging\n",
    "    data_infos[\"train_split_file\"] = \"../train_160.csv\"\n",
    "    data_infos[\"valid_split_file\"] = \"../valid_160.csv\"\n",
    "    data_infos[\"test_split_file\"] = \"../test_160.csv\"\n",
    "else:\n",
    "    data_infos[\"train_split_file\"] = \"random_train_cv{}.csv\".format(fold)\n",
    "    data_infos[\"valid_split_file\"] = \"random_valid_cv{}.csv\".format(fold)\n",
    "    data_infos[\"test_split_file\"] = \"../test_11702.csv\"\n",
    "print(data_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_split_file': '../train_160.csv', 'valid_split_file': '../valid_160.csv', 'test_split_file': '../valid_160.csv', 'model_level_name': 'external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0', 'result_type': 'val', 'predict_aug': 'default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose'}\n"
     ]
    }
   ],
   "source": [
    "data_args = {\n",
    "    \"train_split_file\": data_infos[\"train_split_file\"],\n",
    "    \"valid_split_file\": data_infos[\"valid_split_file\"],\n",
    "    \"test_split_file\": data_infos[\"valid_split_file\"], # should be \"test_split_file?\"\n",
    "    \"model_level_name\": data_infos[\"model_level_name\"],\n",
    "    \"result_type\": \"val\", # for test, should change to 'test'\n",
    "    'predict_aug':predict_aug,\n",
    "}\n",
    "print(data_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Protein class\n",
    "trainer = train_cls_net.Protein(dir_args,\n",
    "                                train_batch_size=train_batch_size,\n",
    "                                test_batch_size=test_batch_size,\n",
    "                                seed=seed, img_size=img_size,in_channels=in_channels,\n",
    "                                save_probs=save_probs,\n",
    "                                aug_version=aug_version,\n",
    "                                num_classes=num_classes,\n",
    "                                crop_size=crop_size,\n",
    "                                use_external=use_external,\n",
    "                                clipnorm=clipnorm,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 4\n"
     ]
    }
   ],
   "source": [
    "# directory for densenet architecture\n",
    "model = import_module(\"net.%s\" % module)\n",
    "\n",
    "# get densenet architecture pretrained on imagenet (model_name = class_densenet121_large_dropout)\n",
    "net, scheduler, loss = model.get_model(model_name,\n",
    "                                       num_classes,\n",
    "                                       loss_name,\n",
    "                                       scheduler_name=scheduler_name,\n",
    "                                       in_channels=in_channels,\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model file from: /home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/result/models/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/final.pth\n",
      "using gpu 0\n"
     ]
    }
   ],
   "source": [
    "# set directories\n",
    "trainer.set_datasets(data_args)\n",
    "\n",
    "# load model located on model file\n",
    "trainer.load_model(net=net, epoch=None) \n",
    "\n",
    "# print model file \n",
    "print('load model file from:', trainer.get_model_file()) \n",
    "\n",
    "# use GPU\n",
    "n_gpu = trainer.setgpu(gpus)\n",
    "net = trainer.set_data_parallel(net, n_gpu=n_gpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_predict_val = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/images_1536\n",
      "/home/ubuntu/HPA/hpa_interp/bestfitting/protein_clean/data/train/external_v18_1536\n",
      "iter:  1\n",
      "iter:  11\n",
      "(160, 28)\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "if is_predict_val:\n",
    "    data_args['result_type'] = 'val'\n",
    "    data_args['test_split_file'] = data_infos[\"valid_split_file\"]\n",
    "    trainer.set_datasets(data_args)\n",
    "\n",
    "    # test dataset and dataloader\n",
    "    test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "                                                   img_size=trainer.img_size,\n",
    "                                                   is_trainset=not trainer.result_type == 'test',\n",
    "                                                   return_label=True,\n",
    "                                                   seed=trainer.seed,\n",
    "                                                   in_channels=trainer.in_channels,\n",
    "                                                   transform=None,\n",
    "                                                   crop_size=trainer.crop_size,\n",
    "                                                   random_crop=trainer.seed!=0,\n",
    "                                                   )\n",
    "\n",
    "    test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "                                             sampler=SequentialSampler(test_dataset),\n",
    "                                             batch_size=trainer.test_batch_size,\n",
    "                                             drop_last=False,\n",
    "                                             num_workers=trainer.num_workers,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    augments = trainer.predict_aug.split(',')\n",
    "    augment_name = augments[2]\n",
    "    # define transform\n",
    "    test_dataset.transform = [eval('augment_%s' % augment_name)]\n",
    "    \n",
    "    epoch_name = 'epoch_final'\n",
    "    augment_name += '_seed%d'%seed\n",
    "    sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "    \n",
    "    if trainer.use_external and trainer.result_type == 'val':\n",
    "        trainer.result_csv_file = opj(sub_dir, 'results_%s_external.csv.gz' % trainer.result_type)\n",
    "        trainer.result_prob_fname = opj(sub_dir, \"prob_%s_external.npy\" % trainer.result_type)\n",
    "    else:\n",
    "        trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % trainer.result_type)\n",
    "        trainer.result_prob_fname = opj(sub_dir, \"prob_%s.npy\" % trainer.result_type)\n",
    "    os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "    # use gpu\n",
    "    if trainer.gpu_flag:\n",
    "        net.cuda()\n",
    "    # net eval mode\n",
    "    net.eval()\n",
    "\n",
    "    n = 0 # number of test data points\n",
    "    img_ids = np.array(test_dataset.img_ids) # get img_ids from dataset\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for iter, (images, labels, indices) in enumerate(test_loader, 0):\n",
    "            if iter % 10 == 1:\n",
    "                print('iter: ', iter)\n",
    "            batch_size = len(images)\n",
    "            n += batch_size\n",
    "            if trainer.gpu_flag:\n",
    "                images = Variable(images.cuda(), volatile=True)\n",
    "            else:\n",
    "                images = Variable(images, volatile=True)\n",
    "\n",
    "            outputs = net(images)\n",
    "            logits = outputs\n",
    "\n",
    "            probs = trainer.logits_to_probs(logits.data)\n",
    "            all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "    # start = timer()\n",
    "\n",
    "    all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "    if trainer.save_probs:\n",
    "        print(all_probs.shape)\n",
    "        np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "    df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "    df.to_csv(trainer.result_csv_file, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feb8e61c-bbc2-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb909d8-bbab-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>febb7ed0-bbc8-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>febc6112-bbad-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>febc967e-bbc0-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>febd613a-bb9d-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>febee7a4-bba6-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>febfd234-bbc6-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fec05240-bb9e-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fec0d0e2-bbbe-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fec0d944-bb9f-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fec21aae-bbc5-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fec21e24-bbb0-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fec454d8-bbbe-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fec59306-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fec7ccf8-bbb9-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fec7f63c-bba1-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fecdd160-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fece0248-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fecf31b6-bbb7-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fed0a594-bbaa-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fed15356-bbc6-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0 21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>fed251bc-bbc5-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fed27984-bba3-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fed441ce-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fed6505c-bbc8-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fed6f4bc-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fed9906a-bbb8-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>feda2ff6-bba5-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fedb49d0-bba6-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ffb00d58-bba3-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>ffb49f82-bbab-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>3 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ffb5effa-bbaf-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>ffb610c6-bbb8-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>ffb6fb76-bba9-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>ffb74ef0-bbae-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ffbebc4e-bb9a-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>ffc30612-bbbf-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>ffc8820a-bbc8-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>7 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>ffcebbc8-bbc5-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>ffced1a2-bbad-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>ffcf84d6-bbac-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>ffd23be4-bba3-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ffd298f4-bbc7-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>ffd2b880-bba8-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>ffd45d0c-bbad-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ffd96262-bbc0-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>ffdb9014-bba2-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>ffdbe064-bbbb-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>ffdcc5e4-bbaa-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>ffe454e6-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>ffe55eba-bbba-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>0 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ffe61798-bbc3-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>ffe8cf0c-bba9-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>ffeae6f0-bbc9-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>ffed4430-bbac-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>fff0a998-bbae-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>fff189d8-bbab-11e8-b2ba-ac1f6b6435d0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0</td>\n",
       "      <td>2 21 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0</td>\n",
       "      <td>0 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Id Predicted\n",
       "0    feb8e61c-bbc2-11e8-b2bc-ac1f6b6435d0      0 25\n",
       "1    feb909d8-bbab-11e8-b2ba-ac1f6b6435d0         3\n",
       "2    febb7ed0-bbc8-11e8-b2bc-ac1f6b6435d0         2\n",
       "3    febc6112-bbad-11e8-b2ba-ac1f6b6435d0        23\n",
       "4    febc967e-bbc0-11e8-b2bb-ac1f6b6435d0       0 1\n",
       "5    febd613a-bb9d-11e8-b2b9-ac1f6b6435d0        23\n",
       "6    febee7a4-bba6-11e8-b2ba-ac1f6b6435d0        25\n",
       "7    febfd234-bbc6-11e8-b2bc-ac1f6b6435d0         0\n",
       "8    fec05240-bb9e-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "9    fec0d0e2-bbbe-11e8-b2ba-ac1f6b6435d0      0 12\n",
       "10   fec0d944-bb9f-11e8-b2b9-ac1f6b6435d0      0 25\n",
       "11   fec21aae-bbc5-11e8-b2bc-ac1f6b6435d0      0 25\n",
       "12   fec21e24-bbb0-11e8-b2ba-ac1f6b6435d0         0\n",
       "13   fec454d8-bbbe-11e8-b2ba-ac1f6b6435d0         5\n",
       "14   fec59306-bbc9-11e8-b2bc-ac1f6b6435d0         3\n",
       "15   fec7ccf8-bbb9-11e8-b2ba-ac1f6b6435d0        23\n",
       "16   fec7f63c-bba1-11e8-b2b9-ac1f6b6435d0        23\n",
       "17   fecdd160-bbc9-11e8-b2bc-ac1f6b6435d0      0 16\n",
       "18   fece0248-bbc4-11e8-b2bc-ac1f6b6435d0         0\n",
       "19   fecf31b6-bbb7-11e8-b2ba-ac1f6b6435d0      0 19\n",
       "20   fed0a594-bbaa-11e8-b2ba-ac1f6b6435d0        23\n",
       "21   fed15356-bbc6-11e8-b2bc-ac1f6b6435d0   0 21 25\n",
       "22   fed251bc-bbc5-11e8-b2bc-ac1f6b6435d0      0 22\n",
       "23   fed27984-bba3-11e8-b2b9-ac1f6b6435d0         7\n",
       "24   fed441ce-bb99-11e8-b2b9-ac1f6b6435d0        22\n",
       "25   fed6505c-bbc8-11e8-b2bc-ac1f6b6435d0        19\n",
       "26   fed6f4bc-bbc3-11e8-b2bc-ac1f6b6435d0        25\n",
       "27   fed9906a-bbb8-11e8-b2ba-ac1f6b6435d0        23\n",
       "28   feda2ff6-bba5-11e8-b2ba-ac1f6b6435d0        23\n",
       "29   fedb49d0-bba6-11e8-b2ba-ac1f6b6435d0         0\n",
       "..                                    ...       ...\n",
       "130  ffb00d58-bba3-11e8-b2b9-ac1f6b6435d0      0 18\n",
       "131  ffb49f82-bbab-11e8-b2ba-ac1f6b6435d0      3 25\n",
       "132  ffb5effa-bbaf-11e8-b2ba-ac1f6b6435d0      0 25\n",
       "133  ffb610c6-bbb8-11e8-b2ba-ac1f6b6435d0      0 11\n",
       "134  ffb6fb76-bba9-11e8-b2ba-ac1f6b6435d0         0\n",
       "135  ffb74ef0-bbae-11e8-b2ba-ac1f6b6435d0       0 7\n",
       "136  ffbebc4e-bb9a-11e8-b2b9-ac1f6b6435d0         4\n",
       "137  ffc30612-bbbf-11e8-b2bb-ac1f6b6435d0       0 3\n",
       "138  ffc8820a-bbc8-11e8-b2bc-ac1f6b6435d0      7 25\n",
       "139  ffcebbc8-bbc5-11e8-b2bc-ac1f6b6435d0         2\n",
       "140  ffced1a2-bbad-11e8-b2ba-ac1f6b6435d0        18\n",
       "141  ffcf84d6-bbac-11e8-b2ba-ac1f6b6435d0         0\n",
       "142  ffd23be4-bba3-11e8-b2b9-ac1f6b6435d0         5\n",
       "143  ffd298f4-bbc7-11e8-b2bc-ac1f6b6435d0         0\n",
       "144  ffd2b880-bba8-11e8-b2ba-ac1f6b6435d0         0\n",
       "145  ffd45d0c-bbad-11e8-b2ba-ac1f6b6435d0      0 25\n",
       "146  ffd96262-bbc0-11e8-b2bb-ac1f6b6435d0         0\n",
       "147  ffdb9014-bba2-11e8-b2b9-ac1f6b6435d0         0\n",
       "148  ffdbe064-bbbb-11e8-b2ba-ac1f6b6435d0        23\n",
       "149  ffdcc5e4-bbaa-11e8-b2ba-ac1f6b6435d0         2\n",
       "150  ffe454e6-bb99-11e8-b2b9-ac1f6b6435d0        16\n",
       "151  ffe55eba-bbba-11e8-b2ba-ac1f6b6435d0       0 5\n",
       "152  ffe61798-bbc3-11e8-b2bc-ac1f6b6435d0        19\n",
       "153  ffe8cf0c-bba9-11e8-b2ba-ac1f6b6435d0        18\n",
       "154  ffeae6f0-bbc9-11e8-b2bc-ac1f6b6435d0         0\n",
       "155  ffed4430-bbac-11e8-b2ba-ac1f6b6435d0        21\n",
       "156  fff0a998-bbae-11e8-b2ba-ac1f6b6435d0         5\n",
       "157  fff189d8-bbab-11e8-b2ba-ac1f6b6435d0         7\n",
       "158  fffdf7e0-bbc4-11e8-b2bc-ac1f6b6435d0   2 21 25\n",
       "159  fffe0ffe-bbc0-11e8-b2bb-ac1f6b6435d0       0 2\n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_predict_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_predict_test:\n",
    "#     data_args['result_type'] = 'test'\n",
    "#     data_args['test_split_file'] = data_infos[\"test_split_file\"]\n",
    "#     trainer.set_datasets(data_args)\n",
    "\n",
    "#     # test dataset and dataloader\n",
    "#     test_dataset = protein_dataset.ProteinDataset(trainer.test_split_file,\n",
    "#                                                    img_size=trainer.img_size,\n",
    "#                                                    is_trainset=not trainer.result_type == 'test',\n",
    "#                                                    return_label=True,\n",
    "#                                                    seed=trainer.seed,\n",
    "#                                                    in_channels=trainer.in_channels,\n",
    "#                                                    transform=None,\n",
    "#                                                    crop_size=trainer.crop_size,\n",
    "#                                                    random_crop=trainer.seed!=0,\n",
    "#                                                    )\n",
    "\n",
    "#     test_loader = protein_dataset.DataLoader(test_dataset,\n",
    "#                                              sampler=SequentialSampler(test_dataset),\n",
    "#                                              batch_size=trainer.test_batch_size,\n",
    "#                                              drop_last=False,\n",
    "#                                              num_workers=trainer.num_workers,\n",
    "#                                              pin_memory=True)\n",
    "\n",
    "#     augments = trainer.predict_aug.split(',')\n",
    "#     augment_name = augments[2]\n",
    "#     # define transform\n",
    "#     test_dataset.transform = [eval('augment_%s' % augment_name)]\n",
    "    \n",
    "#     epoch_name = 'epoch_final'\n",
    "#     augment_name += '_seed%d'%seed\n",
    "#     sub_dir = opj(trainer.subm_dir, epoch_name, augment_name)\n",
    "    \n",
    "#     if trainer.use_external and trainer.result_type == 'val':\n",
    "#         trainer.result_csv_file = opj(sub_dir, 'results_%s_external.csv.gz' % trainer.result_type)\n",
    "#         trainer.result_prob_fname = opj(sub_dir, \"prob_%s_external.npy\" % trainer.result_type)\n",
    "#     else:\n",
    "#         trainer.result_csv_file = opj(sub_dir, 'results_%s.csv.gz' % trainer.result_type)\n",
    "#         trainer.result_prob_fname = opj(sub_dir, \"prob_%s.npy\" % trainer.result_type)\n",
    "#     os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "#     # use gpu\n",
    "#     if trainer.gpu_flag:\n",
    "#         net.cuda()\n",
    "#     # net eval mode\n",
    "#     net.eval()\n",
    "\n",
    "#     n = 0 # number of test data points\n",
    "#     img_ids = np.array(test_dataset.img_ids) # get img_ids from dataset\n",
    "#     all_probs = []\n",
    "#     with torch.no_grad():\n",
    "#         for iter, (images, labels, indices) in enumerate(test_loader, 0):\n",
    "#             if iter % 10 == 1:\n",
    "#                 print('iter: ', iter)\n",
    "#             batch_size = len(images)\n",
    "#             n += batch_size\n",
    "#             if trainer.gpu_flag:\n",
    "#                 images = Variable(images.cuda(), volatile=True)\n",
    "#             else:\n",
    "#                 images = Variable(images, volatile=True)\n",
    "\n",
    "#             outputs = net(images)\n",
    "#             logits = outputs\n",
    "\n",
    "#             probs = trainer.logits_to_probs(logits.data)\n",
    "#             all_probs += probs.cpu().numpy().reshape(-1).tolist() # collect all probs\n",
    "\n",
    "#     # start = timer()\n",
    "\n",
    "#     all_probs = np.array(all_probs).reshape(-1, trainer.num_classes) # all_probs is an array of n-by-num_classes\n",
    "#     if trainer.save_probs:\n",
    "#         print(all_probs.shape)\n",
    "#         np.save(trainer.result_prob_fname, all_probs)\n",
    "\n",
    "#     df = prob_to_result(all_probs, img_ids) # prob_to_result located in net/loss_funcs/kaggle_metric.py; output pd.dataframe of img_ids and pred_list\n",
    "#     df.to_csv(trainer.result_csv_file, index=False, compression='gzip')\n",
    "# print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
